{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbxg-9gJdCxo",
        "outputId": "7e988fd1-23bc-4d7c-80d2-cb4ae2cbaf0a"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[0;32m      2\u001b[0m nltk\u001b[39m.\u001b[39mdownload(\u001b[39m\"\u001b[39m\u001b[39mpunkt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstem\u001b[39;00m \u001b[39mimport\u001b[39;00m PorterStemmer\n",
            "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\__init__.py:153\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtag\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    152\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenize\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m--> 153\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtranslate\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtree\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    155\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msem\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\translate\\__init__.py:24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtranslate\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbleu_score\u001b[39;00m \u001b[39mimport\u001b[39;00m sentence_bleu \u001b[39mas\u001b[39;00m bleu\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtranslate\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mribes_score\u001b[39;00m \u001b[39mimport\u001b[39;00m sentence_ribes \u001b[39mas\u001b[39;00m ribes\n\u001b[1;32m---> 24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtranslate\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmeteor_score\u001b[39;00m \u001b[39mimport\u001b[39;00m meteor_score \u001b[39mas\u001b[39;00m meteor\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtranslate\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m alignment_error_rate\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtranslate\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstack_decoder\u001b[39;00m \u001b[39mimport\u001b[39;00m StackDecoder\n",
            "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\translate\\meteor_score.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mitertools\u001b[39;00m \u001b[39mimport\u001b[39;00m chain, product\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Callable, Iterable, List, Tuple\n\u001b[1;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m WordNetCorpusReader, wordnet\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstem\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m StemmerI\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstem\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mporter\u001b[39;00m \u001b[39mimport\u001b[39;00m PorterStemmer\n",
            "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\__init__.py:64\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mNLTK corpus readers.  The modules in this package provide functions\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mthat can be used to read corpus files in a variety of formats.  These\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreader\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m     65\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyCorpusLoader\n\u001b[0;32m     66\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenize\u001b[39;00m \u001b[39mimport\u001b[39;00m RegexpTokenizer\n",
            "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\reader\\__init__.py:57\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Natural Language Toolkit: Corpus Readers\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Copyright (C) 2001-2023 NLTK Project\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39m# URL: <https://www.nltk.org/>\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m# For license information, see LICENSE.TXT\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mNLTK corpus readers.  The modules in this package provide functions\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mthat can be used to read corpus fileids in a variety of formats.  These\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39misort:skip_file\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreader\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplaintext\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m     58\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreader\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m     59\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreader\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\reader\\plaintext.py:20\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreader\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenize\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m---> 20\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mPlaintextCorpusReader\u001b[39;00m(CorpusReader):\n\u001b[0;32m     21\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39m    Reader for corpora that consist of plaintext documents.  Paragraphs\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m    are assumed to be split using blank lines.  Sentences and words can\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39m    overriding the ``CorpusView`` class variable.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     CorpusView \u001b[39m=\u001b[39m StreamBackedCorpusView\n",
            "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\reader\\plaintext.py:42\u001b[0m, in \u001b[0;36mPlaintextCorpusReader\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m CorpusView \u001b[39m=\u001b[39m StreamBackedCorpusView\n\u001b[0;32m     33\u001b[0m \u001b[39m\"\"\"The corpus view class used by this reader.  Subclasses of\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39m   ``PlaintextCorpusReader`` may specify alternative corpus view\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39m   classes (e.g., to skip the preface sections of documents.)\"\"\"\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     38\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     39\u001b[0m     root,\n\u001b[0;32m     40\u001b[0m     fileids,\n\u001b[0;32m     41\u001b[0m     word_tokenizer\u001b[39m=\u001b[39mWordPunctTokenizer(),\n\u001b[1;32m---> 42\u001b[0m     sent_tokenizer\u001b[39m=\u001b[39mnltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39mLazyLoader(\u001b[39m\"\u001b[39m\u001b[39mtokenizers/punkt/english.pickle\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     43\u001b[0m     para_block_reader\u001b[39m=\u001b[39mread_blankline_block,\n\u001b[0;32m     44\u001b[0m     encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     45\u001b[0m ):\n\u001b[0;32m     46\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[39m    Construct a new plaintext corpus reader for a set of documents\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[39m    located at the given root directory.  Example usage:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39m        corpus into paragraph blocks.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     CorpusReader\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, root, fileids, encoding)\n",
            "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random,json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5TEajvqdyBl"
      },
      "outputs": [],
      "source": [
        "# Using google colab's files method to upload json file which contains the data \n",
        "\n",
        "# from google.colab import files\n",
        "# files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9ubHMBfeEGD"
      },
      "outputs": [],
      "source": [
        "with open(\"intents.json\",'r') as data:\n",
        "  intents = json.load(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tk9AOMB0fXgj",
        "outputId": "7296861f-485e-4580-a42a-3c56b7d8202a"
      },
      "outputs": [],
      "source": [
        "intents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_1vekVUfrGq"
      },
      "outputs": [],
      "source": [
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore = [\"?\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhRrR3AHgXRC"
      },
      "outputs": [],
      "source": [
        "# data preprocessing: looping through json file which is saved in variable \n",
        "# intents and tokenizing the patterns, extract word with it's tag and saving \n",
        "# them in the list.\n",
        "\n",
        "for intent in intents[\"intents\"]:\n",
        "  for pattern in intent[\"patterns\"]:\n",
        "    word = nltk.word_tokenize(pattern)\n",
        "    words.extend(word)\n",
        "\n",
        "    documents.append((word,intent[\"tag\"]))\n",
        "    if intent['tag'] not in classes:\n",
        "      classes.append(intent['tag'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ow76NN8MidyP"
      },
      "outputs": [],
      "source": [
        "words = [stemmer.stem(word.lower()) for word in words if word not in ignore]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "classes = sorted(list(set(classes)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrn1mLTTjHHx",
        "outputId": "0e83997b-4504-4819-8a92-0d07205c2c0f"
      },
      "outputs": [],
      "source": [
        "print(\"Total documents:\", len(documents))\n",
        "print(\"Total classes:\", len(classes))\n",
        "print(\"Total unique words:\", len(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqWFzi5zmOix"
      },
      "outputs": [],
      "source": [
        "# creating lists for training the data\n",
        "\n",
        "training = [] # f(x)\n",
        "output = [] # f(y)\n",
        "\n",
        "output_empty = [0] * len(classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtZBWJEBnRAK",
        "outputId": "a9919f84-768c-4050-9c9f-39efebd0dd17"
      },
      "outputs": [],
      "source": [
        "#test\n",
        "\n",
        "documents[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJKFDuf9nWGS",
        "outputId": "f3362386-7b77-4a93-da38-886747a46afa"
      },
      "outputs": [],
      "source": [
        "for doc in documents:\n",
        "    bag = []\n",
        "    pattern_words = doc[0]\n",
        "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
        "\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "    training.append([bag, output_row])\n",
        "\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aj8ncOWOuZFR"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(10,input_shape=(len(train_x[0]),)))\n",
        "model.add(tf.keras.layers.Dense(10))\n",
        "model.add(tf.keras.layers.Dense(len(train_y[0]), activation='softmax'))\n",
        "model.compile(tf.keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy']) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNOr9PobwPyX",
        "outputId": "f6557ecd-1b2a-4cc0-8a55-a9e16704a6b0"
      },
      "outputs": [],
      "source": [
        "model.fit(np.array(train_x), np.array(train_y), epochs=1000, batch_size=8, verbose=1)\n",
        "model.save(\"model.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pik25VeS9lEY"
      },
      "outputs": [],
      "source": [
        "# prediction\n",
        "\n",
        "import pickle\n",
        "pickle.dump({\"words\":words,\"classes\":classes},open(\"training_data\",\"wb\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agzOqwP4stmb"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "model = load_model(\"model.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Quc0TUgAtces"
      },
      "outputs": [],
      "source": [
        "with open(\"training_data\",\"rb\") as data:\n",
        "  data = pickle.load(data)\n",
        "  words = data[\"words\"]\n",
        "  classes = data[\"classes\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsDzXhXAvLb7"
      },
      "outputs": [],
      "source": [
        "with open(\"intents.json\",'r') as data:\n",
        "  intents = json.load(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fu1FEREYvQVd"
      },
      "outputs": [],
      "source": [
        "def cleanUp(sentence):\n",
        "  sentence_words = nltk.word_tokenize(sentence)\n",
        "  sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
        "  return sentence_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXvZTJZHv1EK"
      },
      "outputs": [],
      "source": [
        "def bow(sentence, words):\n",
        "  sentence_words = cleanUp(sentence)\n",
        "  bag = [0] * len(words)\n",
        "  for s in sentence_words:\n",
        "    for i,w in enumerate(words):\n",
        "      if w==s:\n",
        "        bag[i] = 1\n",
        "  bag = np.array(bag)\n",
        "  return (bag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTf599c2yfhL",
        "outputId": "3ad6d1c2-acf2-47e9-a6af-58b8e4f9bcda"
      },
      "outputs": [],
      "source": [
        "ERR_THRESHOLD = 0.30\n",
        "\n",
        "def classify(sentence):\n",
        "  bag = bow(sentence,words) # generating all the possibilties from a sentence\n",
        "  result = model.predict(np.array([bag]))\n",
        "\n",
        "  result = [[i,r] for i,r in enumerate(result[0])]\n",
        "  result.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "  result_li = []\n",
        "  for res in result:\n",
        "    if res[1] > ERR_THRESHOLD:\n",
        "      result_li.append((classes[res[0]],res[1])) #intent and probability\n",
        "\n",
        "  return result_li\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FosGc4I01qWl"
      },
      "outputs": [],
      "source": [
        "def get_response(sentence):\n",
        "  res = classify(sentence)\n",
        "  if res:\n",
        "    while res:\n",
        "      for intent in intents[\"intents\"]:\n",
        "        if intent[\"tag\"] == res[0][0]:\n",
        "          return print(random.choice(intent[\"responses\"])) \n",
        "\n",
        "      res.pop(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogqxUdap3GSd",
        "outputId": "5b846f72-3c14-4a85-a9d9-8ecfe9cb5182"
      },
      "outputs": [],
      "source": [
        "get_response(\"Who are you?\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
